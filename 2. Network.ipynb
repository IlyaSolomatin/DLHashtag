{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras1.models import Sequential\n",
    "from keras1.layers import Activation, Dropout, Dense, InputLayer\n",
    "#from keras1.layers import core\n",
    "from keras1.layers import recurrent\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim.models.word2vec as w2v\n",
    "from keras_attention_wrapper1 import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#word_vectors = np.load(open('word_vectors.npy'))\n",
    "#with open('twitter_df.pickle', 'rb') as f:\n",
    "#    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweet2vec = w2v.Word2Vec.load(\"tweet2vec.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.60004886e-03,   1.42846018e-01,   4.95475391e-03,\n",
       "        -4.94676903e-02,  -1.93044379e-01,   2.00735971e-01,\n",
       "         3.92130613e-02,  -1.67803898e-01,   1.86922833e-01,\n",
       "         1.46309271e-01,  -4.95591722e-02,  -2.22091392e-01,\n",
       "        -1.94477975e-01,   2.27520373e-02,  -5.01928618e-03,\n",
       "         3.00241411e-01,   7.13465810e-02,  -1.67296838e-03,\n",
       "        -2.95194745e-01,  -1.80402502e-01,  -4.17780429e-01,\n",
       "        -1.71467945e-01,   2.30367035e-02,   2.63082922e-01,\n",
       "        -1.45185977e-01,  -7.85435140e-02,   1.32386252e-01,\n",
       "         1.67865321e-01,  -4.65071857e-01,  -1.46088138e-01,\n",
       "        -8.90424177e-02,   1.66717321e-01,  -5.09386696e-02,\n",
       "        -1.71511993e-01,  -1.69292912e-01,   1.35816723e-01,\n",
       "        -7.71479532e-02,   2.06790879e-01,  -1.08111592e-03,\n",
       "        -2.41935924e-01,  -4.11206603e-01,  -1.29289329e-02,\n",
       "        -2.86620408e-01,  -3.14642906e-01,  -8.26110095e-02,\n",
       "        -1.99181542e-01,  -5.36885634e-02,   2.25668818e-01,\n",
       "         9.21973437e-02,  -1.17519759e-01,  -1.68117851e-01,\n",
       "        -3.47483233e-02,  -1.01499707e-01,  -1.54660225e-01,\n",
       "        -6.56030932e-03,  -4.65145260e-02,   2.18542248e-01,\n",
       "         7.99778569e-03,  -1.82754010e-01,  -2.45743871e-01,\n",
       "        -6.10681362e-02,   3.41767937e-01,  -8.17237124e-02,\n",
       "         5.99746592e-03,   6.72622025e-02,  -9.93807614e-03,\n",
       "        -2.78629065e-02,  -2.02687219e-01,   7.17577413e-02,\n",
       "        -1.34446517e-01,  -1.33284062e-01,   2.02882409e-01,\n",
       "        -1.33872464e-01,   1.07118592e-01,  -7.40522742e-02,\n",
       "        -1.00915074e-01,   1.91704109e-01,   6.32724836e-02,\n",
       "        -2.05698470e-03,   6.59403875e-02,   2.55475581e-01,\n",
       "         2.45255604e-01,   1.66532155e-02,  -2.97577307e-03,\n",
       "         3.43506858e-02,   1.43730626e-01,  -2.60928333e-01,\n",
       "        -1.04451820e-01,   1.42488293e-02,  -1.24918252e-01,\n",
       "        -1.33056447e-01,   1.04629360e-01,   6.08769134e-02,\n",
       "         1.32534817e-01,   1.13608070e-01,  -1.60786420e-01,\n",
       "        -2.12771050e-03,   2.11499304e-01,  -1.04584396e-01,\n",
       "         1.33061633e-01,   1.61734577e-02,  -5.79090305e-02,\n",
       "         1.23251945e-01,   7.44614331e-03,   1.92656144e-02,\n",
       "        -1.69747934e-01,  -9.02805328e-02,   1.28597274e-01,\n",
       "         9.15480852e-02,  -2.00772788e-02,  -5.03100567e-02,\n",
       "        -1.72087401e-01,   9.35821012e-02,  -7.45874494e-02,\n",
       "         4.49451298e-01,  -2.62714952e-01,  -9.02277753e-02,\n",
       "        -3.29722315e-02,   1.31624788e-01,   1.67768955e-01,\n",
       "         1.31176785e-01,  -9.59903225e-02,  -2.29364503e-02,\n",
       "         1.72782373e-02,   1.34072810e-01,  -1.07550338e-01,\n",
       "         3.07177380e-02,   1.01855733e-01,   4.74491119e-02,\n",
       "         4.16896082e-02,   5.35478396e-03,  -1.23265028e-01,\n",
       "        -4.23021731e-04,  -8.28777477e-02,   5.30142896e-02,\n",
       "        -1.41753748e-01,   1.97363961e-02,   1.60487071e-02,\n",
       "         1.40766919e-01,   3.62708909e-03,   5.95502108e-02,\n",
       "        -7.91046247e-02,   2.29590144e-02,   8.79800767e-02,\n",
       "         1.72905758e-01,   2.46479407e-01,  -4.70274575e-02,\n",
       "         6.71728654e-03,   6.95150271e-02,  -7.64717981e-02,\n",
       "        -9.18318331e-02,  -9.91329253e-02,  -6.85482323e-02,\n",
       "         7.93810561e-02,   9.50442851e-02,  -5.75664788e-02,\n",
       "        -4.84661944e-02,  -1.99206591e-01,  -5.22128120e-02,\n",
       "        -1.15829162e-01,  -1.21655837e-01,   5.83552644e-02,\n",
       "        -1.27032846e-01,  -7.18220845e-02,   7.38722607e-02,\n",
       "         3.92755028e-03,   3.93499509e-02,   1.18352026e-01,\n",
       "         3.00448257e-02,  -3.47370878e-02,  -1.29053131e-01,\n",
       "         6.30099745e-03,   5.32761998e-02,   2.24850513e-02,\n",
       "         9.85768996e-03,   8.10659677e-02,   1.34660959e-01,\n",
       "        -1.76845834e-01,  -3.32224071e-02,   7.46573433e-02,\n",
       "         4.28658500e-02,   2.52592359e-02,   5.84442914e-03,\n",
       "         8.13951902e-03,   1.23108253e-01,   3.14699411e-02,\n",
       "         3.06857582e-02,   3.06362193e-02,   1.73782572e-01,\n",
       "         1.48459151e-01,   1.97899789e-01,  -5.82754724e-02,\n",
       "        -1.08076721e-01,   1.71659753e-01,   1.15905635e-01,\n",
       "         9.04703513e-02,  -1.07395813e-01,   1.03880942e-01,\n",
       "         1.52762800e-01,  -1.49149224e-01,   9.04563144e-02,\n",
       "        -7.75392726e-02,   3.82301700e-03,  -1.17401525e-01,\n",
       "         5.03516980e-02,  -3.45044769e-02,  -8.05394500e-02,\n",
       "         6.87119290e-02,  -9.31178182e-02,   3.26510787e-01,\n",
       "        -1.93388954e-01,   1.96497962e-02,   2.24244874e-02,\n",
       "         1.20659344e-01,   1.55777810e-02,  -7.23137334e-02,\n",
       "        -1.66509822e-01,   1.37323454e-01,   1.43491507e-01,\n",
       "         4.00327332e-02,  -1.15297601e-01,   1.06491623e-02,\n",
       "         5.23853004e-02,  -1.37826994e-01,   1.38678476e-01,\n",
       "        -1.97456211e-01,  -1.25753477e-01,  -1.23938829e-01,\n",
       "         4.50511836e-02,   4.62550744e-02,  -8.37234631e-02,\n",
       "         6.15165457e-02,   1.21916890e-01,  -6.75718933e-02,\n",
       "        -1.70538407e-02,   1.59442388e-02,  -1.05271656e-02,\n",
       "        -1.21925980e-01,   1.86113179e-01,   1.67724818e-01,\n",
       "        -1.38174966e-01,   4.60392646e-02,   3.25479507e-02,\n",
       "        -2.15470642e-02,  -9.12691206e-02,  -8.50815773e-02,\n",
       "        -1.02267455e-04,   3.08036251e-04,   1.31449348e-03,\n",
       "        -1.45356819e-01,  -1.15052098e-03,   1.10995129e-01,\n",
       "        -9.07327458e-02,  -4.67855595e-02,  -3.75095308e-02,\n",
       "         1.38624338e-02,  -5.16222864e-02,  -1.73957765e-01,\n",
       "         7.30715394e-02,   8.89691487e-02,  -6.41282201e-02,\n",
       "         2.29626372e-01,   1.51542276e-01,  -1.06211193e-02,\n",
       "        -2.64068004e-02,   9.95711610e-02,   1.54973716e-02,\n",
       "         3.48819532e-02,  -1.47280917e-01,   8.52513388e-02,\n",
       "        -3.93224545e-02,  -3.18602622e-02,   3.08656767e-02,\n",
       "        -4.95970389e-03,   2.04391047e-01,  -6.50010630e-02,\n",
       "        -1.91547647e-01,  -2.34757029e-02,  -1.67291179e-01,\n",
       "        -2.01721400e-01,  -1.42478511e-01,   1.49166659e-01,\n",
       "         2.64685541e-01,  -9.17302296e-02,   1.49419427e-01,\n",
       "        -8.41722637e-02,   1.95903018e-01,  -5.75968549e-02,\n",
       "        -4.93064756e-03,  -3.12921911e-01,   1.07396848e-01,\n",
       "         1.22246541e-01,  -3.14843953e-01,   1.78023756e-01,\n",
       "         2.70637304e-01,   1.29701465e-01,  -2.39260256e-01,\n",
       "         3.02859973e-02,  -9.89774838e-02,   1.43597484e-01,\n",
       "        -3.40792775e-01,   1.91245988e-01,   7.72631615e-02,\n",
       "        -1.46628380e-01,   6.08043186e-02,   1.82284027e-01,\n",
       "         1.56450301e-01,  -1.79155216e-01,  -1.10059962e-01,\n",
       "        -3.06721777e-01,  -2.27346778e-01,   1.64192077e-02,\n",
       "        -5.19784838e-02,  -8.33342299e-02,   1.60665780e-01,\n",
       "         2.43678659e-01,  -1.27004012e-01,   1.16306931e-01,\n",
       "         1.13378353e-01,   7.76913613e-02,  -3.35520834e-01,\n",
       "        -2.02830210e-01,   2.43034922e-02,  -2.24725828e-01,\n",
       "        -1.98027998e-01,   1.41031578e-01,  -6.16242364e-02,\n",
       "         3.04488659e-01,   8.70429352e-02,  -2.52288021e-02,\n",
       "        -5.40015381e-03,   4.23859209e-02,  -5.88705987e-02,\n",
       "         2.76119448e-02,  -1.59738958e-01,   1.91521078e-01,\n",
       "         8.20507556e-02,   5.37887104e-02,   7.61622936e-02,\n",
       "        -8.75020027e-02,  -3.01326454e-01,   1.96386948e-02,\n",
       "         2.66426474e-01,  -1.62667915e-01,  -5.21842912e-02,\n",
       "         8.47260281e-03,  -2.64038682e-01,   1.14720121e-01,\n",
       "        -1.09334752e-01,   1.32193446e-01,   6.12620600e-02,\n",
       "         1.73500463e-01,  -3.95076275e-02,  -1.28766164e-01,\n",
       "        -1.63968131e-01,   1.25503257e-01,   1.16317552e-02,\n",
       "        -3.41399126e-02,  -8.82668793e-02,   3.15311104e-02,\n",
       "        -2.84355164e-01,  -2.00521685e-02,   1.46511104e-02,\n",
       "        -2.90429890e-01,  -2.96659708e-01,   2.43902937e-01,\n",
       "         3.49219441e-02,  -3.47231269e-01,  -3.47818464e-01,\n",
       "        -2.20739082e-01,   8.58839136e-03,   1.08756267e-01,\n",
       "        -1.53354583e-02,   6.77686930e-02,  -2.03978077e-01,\n",
       "        -1.55557260e-01,   9.60646272e-02,  -1.15279101e-01,\n",
       "         1.51571771e-03,  -2.33707577e-01,   6.41427115e-02,\n",
       "        -2.05923077e-02,   1.96695756e-02,   2.17352703e-01,\n",
       "         1.92677937e-02,  -1.76036917e-02,   5.25224581e-02,\n",
       "        -1.12294361e-01,   2.85309702e-01,   6.28066435e-02,\n",
       "        -1.51111307e-02,  -1.70065939e-01,  -1.40148103e-01,\n",
       "        -7.60338828e-02,   8.29509869e-02,   2.38290668e-01,\n",
       "        -2.61222385e-02,   6.08353876e-02,   1.30388271e-02,\n",
       "        -5.57687096e-02,   6.32205904e-02,   1.26499951e-01,\n",
       "         9.28793773e-02,  -5.57295531e-02,   3.78542319e-02,\n",
       "        -2.70426348e-02,   2.19414453e-03,   2.83600669e-02,\n",
       "        -2.05454543e-01,   5.68532273e-02,   1.20977022e-01,\n",
       "        -7.28648901e-02,   5.78179434e-02,  -7.13731125e-02,\n",
       "        -8.01465213e-02,   1.66301224e-02,   1.06742069e-01,\n",
       "        -6.72247335e-02,  -1.39560863e-01,   2.33943164e-02,\n",
       "         8.60649198e-02,  -1.58922732e-01,   8.70410204e-02,\n",
       "        -3.81846726e-02,   2.46786118e-01,   1.11649096e-01,\n",
       "        -8.78342465e-02,   9.85834375e-02,   3.03747598e-02,\n",
       "        -4.57769185e-02,   1.10361412e-01,  -1.11445896e-01,\n",
       "        -5.67241460e-02,   4.41675745e-02,   2.09069215e-02,\n",
       "         1.47480845e-01,  -1.05379634e-01,  -1.09421380e-01,\n",
       "         1.36294097e-01,  -4.60655764e-02,   1.79296285e-01,\n",
       "         1.55153856e-01,  -2.54878123e-05,  -4.34161946e-02,\n",
       "        -3.57396752e-02,  -4.68589962e-02,   1.95943743e-01,\n",
       "         4.38461043e-02,  -4.15529795e-02,   5.34378812e-02,\n",
       "         9.61223096e-02,   1.45431608e-01,   1.44244850e-01,\n",
       "         1.05622336e-01,   1.22993164e-01,  -9.12471563e-02,\n",
       "         1.69806331e-01,  -1.59562051e-01,  -1.22811990e-02,\n",
       "        -1.63281262e-01,  -4.58705761e-02,  -1.27226412e-01,\n",
       "         2.18951866e-01,   5.94047923e-03,   3.78678627e-02,\n",
       "        -1.38472961e-02,   1.97548419e-01,   2.59833783e-01,\n",
       "        -1.17065266e-01,   1.35450423e-01,  -1.80591792e-01,\n",
       "        -1.05915405e-01,   5.75861596e-02,  -2.32343763e-01,\n",
       "        -1.66270494e-01,  -2.54702091e-01,  -2.11661234e-01,\n",
       "        -4.00089324e-02,   5.34465257e-03,   6.70834854e-02,\n",
       "        -6.27289638e-02,   1.32608518e-01,   6.21324480e-02,\n",
       "        -2.35294089e-01,  -1.20389342e-01,   2.57123530e-01,\n",
       "         2.39382356e-01,   1.47409752e-01,   5.08103035e-02,\n",
       "        -2.63821203e-02,  -1.27317965e-01,   8.31713751e-02,\n",
       "        -2.81960458e-01,  -3.07584912e-01,  -1.16091333e-01,\n",
       "        -6.68399259e-02,   1.83992356e-01,   2.19846237e-02,\n",
       "        -2.07562614e-02,   1.67177200e-01,  -1.24681130e-01,\n",
       "        -2.45271027e-01,   1.47217244e-01,   2.04278320e-01,\n",
       "        -3.06308568e-02,  -2.97089517e-01,   2.08127145e-02,\n",
       "         1.34662762e-01,   6.68617412e-02,  -1.80140197e-01,\n",
       "        -1.81473121e-01,  -6.03268966e-02,   1.03220847e-02,\n",
       "         8.50791633e-02,  -2.07661856e-02,  -3.42513323e-02,\n",
       "         5.36312759e-02,   1.47881180e-01,  -8.92669633e-02,\n",
       "         6.16112398e-03,   1.60000905e-01,   1.04176477e-01,\n",
       "        -1.37553826e-01,   5.35458885e-02,   4.01963852e-03,\n",
       "        -1.70209240e-02,   1.56811953e-01,  -1.68914106e-02,\n",
       "         2.24586889e-01,   5.02136648e-02,  -4.13962174e-04,\n",
       "        -7.93788806e-02,  -5.91179691e-02,  -3.20589542e-02,\n",
       "         1.41956136e-01,   1.26480654e-01,   1.14293927e-02,\n",
       "         1.83210999e-01,  -5.33371158e-02,  -2.39372943e-02,\n",
       "        -6.78368239e-03,   1.77814998e-02,   6.41345326e-03,\n",
       "         1.11526093e-02,   8.86417925e-02,   2.15123054e-02,\n",
       "         7.50591457e-02,  -2.64626175e-01,  -1.23370357e-01,\n",
       "        -1.43041700e-01,  -3.27033661e-02,   1.55304214e-02,\n",
       "         4.59468588e-02,  -2.69498629e-03,  -8.34648013e-02,\n",
       "         8.18037242e-02,   4.22977619e-02,   2.70690583e-02,\n",
       "         1.15898326e-01,   3.94676626e-02,  -8.39450490e-03,\n",
       "         1.10345846e-03,  -8.96710008e-02,  -1.32466722e-02,\n",
       "        -6.73080832e-02,  -3.05599630e-01,  -3.11498493e-01,\n",
       "         1.85827225e-01,  -1.59572944e-01,   1.36500701e-01,\n",
       "        -1.89721249e-02,  -2.15065315e-01,  -8.22933987e-02,\n",
       "         9.02213678e-02,  -2.67292392e-02,   6.36795759e-02,\n",
       "         7.35944584e-02,  -1.34999231e-01,  -1.97155066e-02,\n",
       "        -1.84456278e-02,   1.12500019e-01,   1.00177482e-01,\n",
       "        -5.26074283e-02,   1.61096394e-01,  -2.00673789e-01,\n",
       "         1.19501464e-01,   2.31104523e-01,  -1.50695547e-01,\n",
       "         3.15216780e-02,   1.05623506e-01,  -4.49114554e-02,\n",
       "        -1.20264746e-01,  -7.66923577e-02,  -1.13621943e-01,\n",
       "        -9.28462073e-02,  -1.40451148e-01,   1.26333684e-01,\n",
       "         1.38185814e-01,  -2.77023930e-02,   4.60030995e-02], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet2vec.wv['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    vectorized_sentence = []\n",
    "    for word in sentence:\n",
    "        vectorized_sentence.append(tweet2vec.wv[word])\n",
    "    return vectorized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ac9323445eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedding_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m#model.add(Attention(recurrent.LSTM(units=500, return_sequences=True, implementation=1)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "nb_samples, timesteps, embedding_dim, output_dim = 2, 5, 3, 4 #32, 5, 600, 4\n",
    "embedding_num = 12\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(batch_input_shape=(nb_samples, timesteps, embedding_dim)))\n",
    "#model.add(Attention(recurrent.LSTM(units=500, return_sequences=True, implementation=1)))\n",
    "#model.add(Attention(recurrent.LSTM(units=500, return_sequences=True, consume_less='mem')))\n",
    "model.add(Attention(recurrent.LSTM(output_dim, input_dim=embedding_dim, return_sequences=True, consume_less='mem')))\n",
    "model.add(Dense(256, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the code from https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "data_train = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
    "print data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bd2047172373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for idx in range(data_train.review.shape[0]):\n",
    "    text = BeautifulSoup(data_train.review[idx])\n",
    "    text = clean_str(text.get_text().encode('ascii','ignore'))\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "    \n",
    "    labels.append(data_train.sentiment[idx])\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1                    \n",
    "                    \n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"/twitter\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a9fb2f89d0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0membedding_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[1;31m# words not found in embedding index will be all-zeros.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "preds = Dense(2, activation='softmax')(l_lstm_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "print model.summary()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to adapt this code to our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "data_train = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
    "print data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for idx in range(data_train.review.shape[0]):\n",
    "    text = BeautifulSoup(data_train.review[idx])\n",
    "    text = clean_str(text.get_text().encode('ascii','ignore'))\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "    \n",
    "    labels.append(data_train.sentiment[idx])\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1                    \n",
    "                    \n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"/twitter\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "preds = Dense(2, activation='softmax')(l_lstm_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "print model.summary()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
